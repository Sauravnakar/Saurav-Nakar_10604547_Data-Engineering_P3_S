services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  spark:
    image: apache/spark:3.5.1
    command: ["sleep", "infinity"]
    volumes:
      - ./data:/data
      - ./processing:/processing
      - ./aggregation:/aggregation

  airflow:
    image: apache/airflow:2.7.0
    container_name: airflow
    user: "0:0"
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow/airflow.db
      AIRFLOW__CORE__FERNET_KEY: ""
    volumes:
    - ./airflow:/opt/airflow
    ports:
    - "8081:8080"
    command: >
      bash -c "
      mkdir -p /opt/airflow/airflow &&
      airflow db init &&
      airflow users create
      --username airflow
      --password airflow
      --firstname admin
      --lastname user
      --role Admin
      --email admin@example.com || true &&
      airflow scheduler & airflow webserver
      "




